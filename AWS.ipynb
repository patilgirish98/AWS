{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9C7WkxoGU0Z3"
      },
      "outputs": [],
      "source": [
        "# Explain the difference between AWS Regions, Availability Zones, and Edge Locations. Why is this important for data analysis and latency-sensitive applications\"\n",
        "\n",
        "   - AWS Regions\n",
        "      - Definition: A Region is a geographically distinct area that contains multiple data centers called Availability Zones (AZs).\n",
        "      - Purpose: Regions allow customers to deploy resources close to their users or to meet regulatory and compliance requirements.\n",
        "   - Availability Zones (AZs)\n",
        "      - Definition: An Availability Zone is one or more physically separate data centers within a region, each with independent power, cooling, and networking.\n",
        "      - Purpose: AZs offer high availability and fault tolerance. Services like EC2, RDS, and EBS can be deployed across AZs to ensure uptime during failures.\n",
        "   - Edge Locations\n",
        "      - Definition: Edge Locations are endpoints of the AWS global network used for content delivery and low-latency access, primarily used by Amazon CloudFront, Route 53, and AWS Global Accelerator.\n",
        "      - Purpose: Serve cached/static content to users as quickly as possible, reducing latency.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the AWS CLI, list all available AWS regions. Share the command used and the output\n",
        "\n",
        "aws ec2 describe-regions --query \"Regions[*].RegionName\" --output table\n"
      ],
      "metadata": {
        "id": "tMNAU0CgWGd4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new IAM user with least privilege access to Amazon S3. Share your attached policies (JSON or screenshot)\n",
        "\n",
        "     1. 1. Create the IAM User (AWS CLI)\n",
        "          aws iam create-user --user-name s3-least-priv-user\n",
        "\n",
        "     2. Attach Inline Policy to the User\n",
        "          {\n",
        "  \"Version\": \"2012-10-17\",\n",
        "  \"Statement\": [\n",
        "    {\n",
        "      \"Sid\": \"AllowMinimalS3Access\",\n",
        "      \"Effect\": \"Allow\",\n",
        "      \"Action\": [\n",
        "        \"s3:ListBucket\"\n",
        "      ],\n",
        "      \"Resource\": [\n",
        "        \"arn:aws:s3:::your-bucket-name\"\n",
        "      ]\n",
        "    },\n",
        "    {\n",
        "      \"Sid\": \"AllowLimitedObjectAccess\",\n",
        "      \"Effect\": \"Allow\",\n",
        "      \"Action\": [\n",
        "        \"s3:GetObject\",\n",
        "        \"s3:PutObject\"\n",
        "      ],\n",
        "      \"Resource\": [\n",
        "        \"arn:aws:s3:::your-bucket-name/*\"\n",
        "      ]\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "       3. Attach the Policy to the User\n",
        "           aws iam put-user-policy \\\n",
        "  --user-name s3-least-priv-user \\\n",
        "  --policy-name S3LeastPrivilegePolicy \\\n",
        "  --policy-document file://s3-least-privilege-policy.json\n",
        "\n",
        "\n",
        "\n",
        "#  Compare different Amazon S3 storage (Standard, Intelligent-Tiering, Glacier). When should each be used in data analytics workflows\n",
        "\n",
        "    - 1. S3 Standard\n",
        "       - Use Case: Frequently accessed data (hot data).\n",
        "       - Durability: 99.999999999% (11 nines).\n",
        "       - Availability: 99.99%\n",
        "       - Latency: Milliseconds\n",
        "    - 2. S3 Intelligent-Tiering\n",
        "       - Use Case: Data with unknown or unpredictable access patterns.\n",
        "       - Durability: 99.999999999%\n",
        "       - Availability: 99.9â€“99.99%\n",
        "       - Latency: Milliseconds (for frequent + infrequent tiers)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "n6ZL6IceWVym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an S3 bucket and upload a sample dataset (CSV or JSON). Enable versioning and show at least two versions of one file\n",
        "\n",
        "     - Step 1: Create an S3 Bucket\n",
        "        aws s3api create-bucket \\\n",
        "  --bucket your-unique-bucket-name \\\n",
        "  --region us-east-1\n",
        "\n",
        "     - Step 2: Enable Versioning\n",
        "         aws s3api put-bucket-versioning \\\n",
        "  --bucket your-unique-bucket-name \\\n",
        "  --versioning-configuration Status=Enabled\n",
        "\n",
        "\n",
        "     - Step 3: Create a Sample CSV File\n",
        "         echo \"id,name,value\" > sample.csv\n",
        "echo \"1,Alice,100\" >> sample.csv\n",
        "\n",
        "     - Step 4: List File Version\n",
        "        aws s3api list-object-versions --bucket your-unique-bucket-name --prefix sample.csv\n",
        "\n",
        "\n",
        "     - Sample Output (simplified):\n",
        "\n",
        "        {\n",
        "  \"Versions\": [\n",
        "    {\n",
        "      \"VersionId\": \"A1B2C3D4E5F6G7\",\n",
        "      \"Key\": \"sample.csv\",\n",
        "      \"LastModified\": \"2025-05-29T12:00:00.000Z\",\n",
        "      \"IsLatest\": true\n",
        "    },\n",
        "    {\n",
        "      \"VersionId\": \"X7Y8Z9W0V1U2T3\",\n",
        "      \"Key\": \"sample.csv\",\n",
        "      \"LastModified\": \"2025-05-29T11:58:00.000Z\",\n",
        "      \"IsLatest\": false\n",
        "    }\n",
        "  ]\n",
        "}\n"
      ],
      "metadata": {
        "id": "cmGH594zXrbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write and apply a lifecycle policy to move files to Glacier after 30 days and delete them after 90. Share the policy JSON\n",
        "\n",
        "    - Step 1: Lifecycle Policy JSON\n",
        "\n",
        "        {\n",
        "  \"Rules\": [\n",
        "    {\n",
        "      \"ID\": \"MoveToGlacierAndDelete\",\n",
        "      \"Filter\": {\n",
        "        \"Prefix\": \"\"\n",
        "      },\n",
        "      \"Status\": \"Enabled\",\n",
        "      \"Transitions\": [\n",
        "        {\n",
        "          \"Days\": 30,\n",
        "          \"StorageClass\": \"GLACIER\"\n",
        "        }\n",
        "      ],\n",
        "      \"Expiration\": {\n",
        "        \"Days\": 90\n",
        "      }\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "     - Step 2: Apply the Lifecycle Policy\n",
        "\n",
        "     aws s3api put-bucket-lifecycle-configuration \\\n",
        "  --bucket your-unique-bucket-name \\\n",
        "  --lifecycle-configuration file://s3-lifecycle-policy.json\n",
        "\n",
        "    - Step 3: Verify the Lifecycle Policy\n",
        "\n",
        "    aws s3api get-bucket-lifecycle-configuration \\\n",
        "  --bucket your-unique-bucket-name\n",
        "\n",
        "\n",
        "\n",
        "# Compare RDS, DynamoDB, and Redshift for use in different stages of a data pipeline. Give one use case for each\n",
        "\n",
        "   - RDS\n",
        "       - Relational DB (SQL)\n",
        "       - Transactional data (OLTP)\n",
        "       - Structured SQL\n",
        "       - Vertical scaling\n",
        "   - DynamoDB\n",
        "       - NoSQL (Key-Value / JSON)\n",
        "       - High-speed, low-latency apps\n",
        "       - Key-based queries\n",
        "       - Auto-scaling\n",
        "   - Redshift\n",
        "       - Columnar Data Warehouse\n",
        "       - Large-scale analytics (OLAP)\n",
        "       - Complex joins, aggreg.\n",
        "       - Horizontal scaling"
      ],
      "metadata": {
        "id": "svE2cpIzYUwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DynamoDB table and insert 3 records manually. Then write a Lambda function that adds records when triggered by S3 uploads\n",
        "\n",
        "   - Step 1: Create a DynamoDB Table\n",
        "\n",
        "       aws dynamodb create-table \\\n",
        "  --table-name UserActivity \\\n",
        "  --attribute-definitions AttributeName=UserId,AttributeType=S \\\n",
        "  --key-schema AttributeName=UserId,KeyType=HASH \\\n",
        "  --provisioned-throughput ReadCapacityUnits=5,WriteCapacityUnits=5\n",
        "\n",
        "\n",
        "  - Step 2: Insert 3 Records Manually\n",
        "\n",
        "      aws dynamodb put-item \\\n",
        "  --table-name UserActivity \\\n",
        "  --item '{\"UserId\": {\"S\": \"user1\"}, \"Activity\": {\"S\": \"Login\"}, \"Timestamp\": {\"S\": \"2025-05-29T10:00:00Z\"}}'\n",
        "\n",
        "aws dynamodb put-item \\\n",
        "  --table-name UserActivity \\\n",
        "  --item '{\"UserId\": {\"S\": \"user2\"}, \"Activity\": {\"S\": \"ViewPage\"}, \"Timestamp\": {\"S\": \"2025-05-29T10:02:00Z\"}}'\n",
        "\n",
        "aws dynamodb put-item \\\n",
        "  --table-name UserActivity \\\n",
        "  --item '{\"UserId\": {\"S\": \"user3\"}, \"Activity\": {\"S\": \"Purchase\"}, \"Timestamp\": {\"S\": \"2025-05-29T10:05:00Z\"}}'\n",
        "\n",
        "\n",
        "   - Step 3: Lambda Function to Add Records on S3 Upload\n",
        "\n",
        "       import json\n",
        "import boto3\n",
        "from datetime import datetime\n",
        "\n",
        "dynamodb = boto3.resource('dynamodb')\n",
        "table = dynamodb.Table('UserActivity')\n",
        "\n",
        "def lambda_handler(event, context):\n",
        "    for record in event['Records']:\n",
        "        bucket = record['s3']['bucket']['name']\n",
        "        key = record['s3']['object']['key']\n",
        "        size = record['s3']['object']['size']\n",
        "\n",
        "        # Generate a fake UserId for demo purposes\n",
        "        user_id = f\"user_{key.split('.')[0]}\"\n",
        "\n",
        "        # Put item in DynamoDB\n",
        "        table.put_item(\n",
        "            Item={\n",
        "                'UserId': user_id,\n",
        "                'Activity': f\"Uploaded {key}\",\n",
        "                'Timestamp': datetime.utcnow().isoformat(),\n",
        "                'FileSize': str(size)\n",
        "            }\n",
        "        )\n",
        "\n",
        "    return {\n",
        "        'statusCode': 200,\n",
        "        'body': json.dumps('DynamoDB record created from S3 upload')\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "#  What is serverless computing? Discuss pros and cons of using AWS Lambda for data pipelines\n",
        "\n",
        "    - What is Serverless Computing?\n",
        "       - Serverless computing is a cloud computing execution model where the cloud provider dynamically manages the allocation and provisioning of servers.\n",
        "       - Instead of managing servers or infrastructure, developers write and deploy code, and the cloud automatically handles scaling, maintenance, and resource allocation.\n",
        "    - Pros of Using AWS Lambda for Data Pipelines\n",
        "       - No Server Management;\n",
        "       - Automatic Scaling:\n",
        "       - Cost Efficiency:\n",
        "       - Event-driven Architecture:\n",
        "       - Fast Deployment\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Bwsy9MpeZnHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Lambda function triggered by S3 uploads that logs file name, size, and timestamp to Cloudwatch. Share code and a log screenshot\n",
        "\n",
        "   - Step 1: Lambda Function Code (Python)\n",
        "\n",
        "        import json\n",
        "import logging\n",
        "import boto3\n",
        "from datetime import datetime\n",
        "\n",
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "def lambda_handler(event, context):\n",
        "    # The event contains info about the S3 object(s)\n",
        "    for record in event['Records']:\n",
        "        bucket_name = record['s3']['bucket']['name']\n",
        "        object_key = record['s3']['object']['key']\n",
        "        object_size = record['s3']['object']['size']\n",
        "        event_time = record['eventTime']  # ISO 8601 string\n",
        "\n",
        "        # Log the details\n",
        "        logger.info(f\"File uploaded - Bucket: {bucket_name}, Key: {object_key}, Size: {object_size} bytes, Time: {event_time}\")\n",
        "\n",
        "    return {\n",
        "        'statusCode': 200,\n",
        "        'body': json.dumps('Logged S3 upload details.')\n",
        "    }\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rt6mK9sHaywv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Use AWS Glue to crawl your S3 dataset, create a Data Catalog table, and run a Glue job to convert CSV data to parquet. Share job code and output location\n",
        "\n",
        "\n",
        "     import sys\n",
        "from awsglue.transforms import *\n",
        "from awsglue.utils import getResolvedOptions\n",
        "from awsglue.context import GlueContext\n",
        "from awsglue.job import Job\n",
        "from pyspark.context import SparkContext\n",
        "\n",
        "# Parameters passed when starting the job\n",
        "args = getResolvedOptions(sys.argv, ['JOB_NAME', 'source_database', 'source_table', 'target_path'])\n",
        "\n",
        "sc = SparkContext()\n",
        "glueContext = GlueContext(sc)\n",
        "spark = glueContext.spark_session\n",
        "job = Job(glueContext)\n",
        "job.init(args['JOB_NAME'], args)\n",
        "\n",
        "# Read the data from Glue Catalog (CSV source)\n",
        "datasource0 = glueContext.create_dynamic_frame.from_catalog(\n",
        "    database=args['source_database'],\n",
        "    table_name=args['source_table'],\n",
        "    transformation_ctx=\"datasource0\"\n",
        ")\n",
        "\n",
        "# Convert to Parquet and write to S3\n",
        "glueContext.write_dynamic_frame.from_options(\n",
        "    frame=datasource0,\n",
        "    connection_type=\"s3\",\n",
        "    connection_options={\"path\": args['target_path']},\n",
        "    format=\"parquet\",\n",
        "    transformation_ctx=\"datasink0\"\n",
        ")\n",
        "\n",
        "job.commit()\n",
        "\n",
        "\n",
        "\n",
        "# Explain the difference between Kinesis Data Streams, Kinesis Firehose, and Kinesis Data Analytics. Provide a real-world example of how each would be used\n",
        "\n",
        "   - Kinesis Data Streams\n",
        "      - A scalable, low-latency data streaming service for collecting and processing large streams of data records in real time.\n",
        "      - It allows you to build custom applications that consume and process streaming data with fine-grained control.\n",
        "      - When you need real-time, custom processing and buffering of streaming data with the ability to replay data.\n",
        "      - A stock trading platform that collects real-time price tick data from exchanges and feeds it into a custom application for live fraud detection or trend analysis.\n",
        "   - Kinesis Data Firehose\n",
        "      - When you want a simple, managed way to capture and store streaming data without managing the underlying infrastructure or building custom applications.\n",
        "      - Streaming website clickstream logs directly into Amazon S3 for batch analytics and storage, without building a custom streaming pipeline.\n",
        "   - Kinesis Data Analytics\n",
        "      - When you want to perform real-time analytics or monitoring with SQL queries on streaming data.\n",
        "      - Real-time dashboard monitoring user activity where you want to detect unusual patterns by querying clickstream data with SQL as it flows in.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# What is columnar storage and how does it benefit Redshift performance for analytics workloads\n",
        "\n",
        "    - What is Columnar Storage?\n",
        "        - Columnar storage is a method of storing data in a database where data is stored column-by-column instead of row-by-row.\n",
        "        - Instead of storing complete rows together, each columnâ€™s data is stored contiguously on disk.\n",
        "    - How Does Columnar Storage Benefit Amazon Redshift Performance for Analytics?\n",
        "        - Efficient I/O for Analytics Queries\n",
        "        - Better Compression\n",
        "        - Faster Aggregations and Scans\n",
        "        - Improved CPU Efficiency\n",
        "\n"
      ],
      "metadata": {
        "id": "Gsf0GWvsbCU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Load a CSV file from S3 into Redshift using the COPY command. Share table schema, command used, and sample output from a query\n",
        "\n",
        "   -  Heres a corresponding Redshift table schema:\n",
        "\n",
        "\n",
        "        CREATE TABLE sales (\n",
        "    sale_id INT,\n",
        "    sale_date DATE,\n",
        "    customer_id INT,\n",
        "    amount DECIMAL(10, 2)\n",
        ");\n",
        "\n",
        "   - COPY Command to Load CSV from S3\n",
        "\n",
        "     COPY sales\n",
        "FROM 's3://my-bucket/data/sales.csv'\n",
        "IAM_ROLE 'arn:aws:iam::123456789012:role/MyRedshiftRole'\n",
        "CSV\n",
        "IGNOREHEADER 1\n",
        "DELIMITER ','\n",
        "TIMEFORMAT 'auto';\n",
        "\n",
        "\n",
        "\n",
        "#  What is the role of the AWS Glue Data Catalog in Athena? How does schema-on-read work\n",
        "\n",
        "   - Role of AWS Glue Data Catalog in Amazon Athena\n",
        "      - The AWS Glue Data Catalog is a centralized metadata repository that stores table definitions, schemas, and location information for your data.\n",
        "      - Athena uses the Glue Data Catalog as its metadata store to understand\n",
        "      - When you run a query in Athena, it refers to the Glue Data Catalog to parse and interpret the underlying raw data\n",
        "      - Glue Data Catalog can also be used by other AWS analytics services (like Redshift Spectrum, Glue ETL, EMR), enabling consistent metadata management across AWS.\n",
        "\n",
        "   - How Schema-on-Read Works\n",
        "      - Schema-on-read means the schema is applied when you query the data, not when you write or store it\n",
        "      - Unlike traditional databases (schema-on-write) where data must conform to a schema before storing, schema-on-read lets you store raw data as-is (like CSV, JSON, Parquet) in S3."
      ],
      "metadata": {
        "id": "bEwcHP9wcdMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an Athena table from S3 data using Glue Catalog. Run a query and share the SQL + result screenshoV\n",
        "\n",
        "   tep 1: Create Glue Catalog Table (using Athena SQL)\n",
        "      CREATE EXTERNAL TABLE IF NOT EXISTS sales (\n",
        "  sale_id INT,\n",
        "  sale_date STRING,\n",
        "  customer_id INT,\n",
        "  amount DOUBLE\n",
        ")\n",
        "ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\n",
        "WITH SERDEPROPERTIES (\n",
        "  \"separatorChar\" = \",\",\n",
        "  \"quoteChar\"     = \"\\\"\",\n",
        "  \"escapeChar\"    = \"\\\\\"\n",
        ")\n",
        "LOCATION 's3://my-bucket/data/'\n",
        "TBLPROPERTIES ('has_encrypted_data'='false');\n",
        "\n",
        "   Step 2: Run a Sample Query in Athena\n",
        "\n",
        "     SELECT sale_id, sale_date, amount\n",
        "FROM sales\n",
        "WHERE amount > 100\n",
        "ORDER BY sale_date DESC\n",
        "LIMIT 5;\n",
        "\n",
        "\n",
        "\n",
        "# Describe how Amazon Quicksight supports business intelligence in a serverless data architecture. What are SPICE and embedded dashboards\n",
        "\n",
        "   - How Amazon QuickSight Supports BI in a Serverless Data Architecture\n",
        "      - Amazon QuickSight is a fully managed, cloud-native business intelligence (BI) service that lets you visualize and analyze data without managing any infrastructure (hence, serverless).\n",
        "      - It connects directly to various AWS data sources like S3, Redshift, Athena, RDS, and more â€” no need to provision servers or manage scaling.\n",
        "      - QuickSight automatically scales to accommodate users and data size, charging you based on usage, so you only pay for what you use.\n",
        "      - It simplifies BI by enabling users to create interactive dashboards and visualizations quickly, with minimal setup.\n",
        "  - What are Embedded Dashboards?\n",
        "      - Embedded Dashboards let you integrate QuickSight dashboards into your own web applications or portals.\n",
        "      - This provides your customers or internal users with interactive BI reports without them needing a separate QuickSight login.\n",
        "      - You can embed dashboards securely, control access, and customize the look and feel.\n",
        "      - Ideal for SaaS vendors or internal apps wanting to provide BI as part of the user experience.\n",
        "\n",
        "\n",
        "# Explain how AWS CloudWatch and CloudTrail differ. IN a data analytics pipeline, what role does each play in monitoring, auditing, and troubleshooting\n",
        "\n",
        "   - AWS CloudWatch\n",
        "     - \tMonitoring and observability of AWS resources\n",
        "     - \tMetrics (CPU, memory, network), logs, alarms\n",
        "     - \tOperational data: performance metrics & logs\n",
        "     - Yes â€” for alerting, dashboards, auto-scaling\n",
        "   - \tAWS CloudTrail\n",
        "     - Governance, auditing, and compliance tracking\n",
        "     - API calls made to AWS services (who did what)\n",
        "     - Security and operational audit trails\n",
        "     - Mostly post-facto analysis and auditing\n",
        "\n",
        "\n",
        "# Describe a complete end-to-end data analytics pipeline using AWS services. (Example: S3 â†’ Lambda â†’ Glue â†’ Quicksight) Explain why you would choose each service for the stage its used in\n",
        "\n",
        "   1. Data Ingestion: Amazon Kinesis Data Firehose\n",
        "      - Kinesis Firehose provides a fully managed, scalable, and easy-to-configure data ingestion service that automatically captures streaming data and delivers it to storage destinations like S3.\n",
        "      - It supports data buffering, compression, and transformation (via Lambda), allowing near real-time data ingestion with minimal operational overhead.\n",
        "      - Use case: Collect streaming data such as website clickstreams, IoT sensor data, or application logs.\n",
        "   2. Data Storage: Amazon S3\n",
        "      - S3 is a highly durable, cost-effective, and scalable data lake storage service. It can store raw data of any format (CSV, JSON, Parquet) with virtually unlimited size\n",
        "      - Use case: Store raw streaming data ingested from Firehose, as well as transformed and partitioned datasets for downstream analytics.\n",
        "   3. Data Catalog & Transformation: AWS Glue\n",
        "      - Glue automatically crawls your S3 data to create and maintain a centralized Data Catalog, capturing table schemas and partitions.\n",
        "      - Use case: Clean and transform raw data to optimized, query-efficient formats; keep metadata updated for querying services.\n",
        "   4. Querying: Amazon Athena\n",
        "      - Athena is a serverless interactive query service that allows you to run standard SQL queries directly on data stored in S3 using the Glue Data Catalog metadata.\n",
        "      - Use case: Analysts and data scientists can explore and analyze datasets ad hoc without needing to move or load data elsewhere.\n",
        "   5. Visualization: Amazon QuickSight\n",
        "      - QuickSight is a fully managed BI and visualization service that integrates seamlessly with Athena and Glue Data Catalog\n",
        "      - Use case: Business users and stakeholders visualize analytics results with rich, shareable dashboards embedded in applications or portals."
      ],
      "metadata": {
        "id": "6bvyIEQWdTLo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}